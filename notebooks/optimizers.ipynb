{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will implement the Adam optimizer which combines the best properties of AdaGrad and RMSProp optimizers to improve the performance when training deep learning models. \n",
    "\n",
    "Why do we need optimizers? \n",
    "\n",
    "In neural networks, the goal is to minimize the loss function, which measures how far the networkÂ´s predicted values are from the actual values. This is done by adjusting the networks parameters (weights and biases) a process called training. Optimizers are used to make this training more process more efficient. Without optimizers we have to manually guide the parameters towards a minimum of the loss function - this is not feasible in high dimensional spaces with millions of parameters. By manually guiding the parameters I mean being blindfolded in a forest and trying to find your way to the cheapest hotel. The optimizer is any form of plan or help we have to find our desired destination. An optimizer looks at the gradients generated in the process of backpropagation (propagating the error backwards through the network).\n",
    "\n",
    "You might have heard of gradient descent or stochastic gradient descent. Gradient descent is the most basic form of optimizer. It uses the gradient of the loss function together with a learning rate to update parameters. The learning rate is basically how much we allow the gradient of the loss to influence the new update of the parameters. Traditional gradient descent however suffers from the vanishing gradient problem - where the gradients during backpropagation (which are used to update the parameters) become too small. This results in the weights in earlier layers being updated very slowly (small gradients mean small updates) and this effectively stalls the training process which becomes slower and slower. The deeper the network, the more severe the problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
